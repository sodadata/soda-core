import logging
import re
from datetime import datetime
from typing import Optional

import boto3
from soda_athena.common.data_sources.athena_data_source_connection import (
    AthenaDataSource as AthenaDataSourceModel,
)
from soda_athena.common.data_sources.athena_data_source_connection import (
    AthenaDataSourceConnection,
)
from soda_core.common.data_source_connection import DataSourceConnection
from soda_core.common.data_source_impl import DataSourceImpl
from soda_core.common.data_source_results import UpdateResult
from soda_core.common.logging_constants import soda_logger
from soda_core.common.sql_ast import COLUMN, CREATE_TABLE, CREATE_TABLE_IF_NOT_EXISTS
from soda_core.common.sql_datatypes import DBDataType
from soda_core.common.sql_dialect import SqlDialect
from soda_core.common.statements.metadata_columns_query import MetadataColumnsQuery

logger: logging.Logger = soda_logger


class AthenaDataSourceImpl(DataSourceImpl, model_class=AthenaDataSourceModel):
    def __init__(self, data_source_model: AthenaDataSourceModel):
        super().__init__(data_source_model=data_source_model)

    def _create_sql_dialect(self) -> SqlDialect:
        return AthenaSqlDialect(self)

    def _create_data_source_connection(self) -> DataSourceConnection:
        return AthenaDataSourceConnection(
            name=self.data_source_model.name, connection_properties=self.data_source_model.connection_properties
        )

    def create_metadata_columns_query(self) -> MetadataColumnsQuery:
        return MetadataColumnsQuery(
            sql_dialect=self.sql_dialect,
            data_source_connection=self.connection,
            dataset_name_casify=True,
        )

    # def execute_query(self, sql: str) -> QueryResult:
    #     # Athena does not play well with freezegun.
    #     # The datasources requires a timestamp in the query (done automatically), this timestamp must be in sync with AWS servers
    #     # This is not the case when using freezegun.
    #     # We need to disable freezegun for this method.

    #     # TODO: Refactor this so it's in a proper place. It shouldn't live here.
    #     # now = datetime.now()
    #     # if now < datetime(year=2025, month=8, day=1, hour=0, minute=0, second=0):
    #     #     with freezegun.freeze_time(freezegun.api.real_datetime.now(tz=timezone.utc)):
    #     #         result = self.connection.execute_query(sql=sql)
    #     #     return result
    #     return self.connection.execute_query(sql=sql)

    def execute_update(self, sql: str) -> UpdateResult:
        result: UpdateResult = super().execute_update(sql)
        # Athena requires some extra cleanup for a drop table or drop schema statement.
        if sql.startswith("DROP TABLE "):
            # We assume that the statement is generated by the SQL Dialect, so we can parse the table name from it.
            qualified_table_name = self.__parse_table_name_from_drop_table_sql(sql)
            table_location = self.table_s3_location(qualified_table_name)
            self._delete_s3_files(table_location)
        elif sql.startswith("DROP SCHEMA"):
            pass  # This is for later, if we ever support dropping schemas in the SQL AST.
        return result

    def table_s3_location(self, qualified_table_name: str) -> str:
        table_part_for_location = qualified_table_name.replace(".", "/").replace('"', "")
        location = f"{self.connection.athena_staging_dir}/{table_part_for_location}/"
        return location

    def __parse_table_name_from_drop_table_sql(self, sql: str) -> str:
        remaining_sql = sql[len("DROP TABLE ") :].strip()
        if remaining_sql.startswith("IF EXISTS "):
            remaining_sql = remaining_sql[len("IF EXISTS ") :].strip()
        if remaining_sql.endswith(";"):
            remaining_sql = remaining_sql[:-1]
        return remaining_sql.strip()

    def _delete_s3_files(self, directory: str):
        """
        Deletes all files under a given directory in S3.
        """
        logger.debug(f"Deleting all s3 files under {directory}")
        bucket = self._extract_s3_bucket(directory)
        folder = self._extract_s3_folder(directory)
        s3_client = self._create_s3_client()
        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=folder)
        object_keys = self._extract_object_keys(response)
        logger.debug(f"Found {len(object_keys)} to be deleted")
        max_objects = 200
        assert len(object_keys) < max_objects, (
            f"This method is intended for tests and hence limited to a maximum of {max_objects} objects, "
            f"{len(object_keys)} objects exceeds the limit."
        )
        if len(object_keys) > 0:
            response: dict = s3_client.delete_objects(Bucket=bucket, Delete={"Objects": object_keys})
            deleted_list = response.get("Deleted") if isinstance(response, dict) else None
            deleted_count = (
                len(deleted_list) if isinstance(deleted_list, list) else "Unknown aws delete objects response"
            )
            logger.debug(f"Deleted {deleted_count}")

    def _create_s3_client(self) -> boto3.client:
        self.filter_false_positive_boto3_warning()
        aws_credentials = self.connection.aws_credentials
        aws_credentials = aws_credentials.resolve_role("soda_sql_test_cleanup")
        return boto3.client(
            "s3",
            region_name=aws_credentials.region_name,
            aws_access_key_id=aws_credentials.access_key_id,
            aws_secret_access_key=aws_credentials.secret_access_key,
            aws_session_token=aws_credentials.session_token,
        )

    def _extract_object_keys(self, response: dict) -> list[dict]:
        object_keys = []
        if "Contents" in response:
            objects = response["Contents"]
            for summary in objects:
                key = summary["Key"]
                object_keys.append({"Key": key})
        return object_keys

    S3_URI_PATTERN = r"(^s3://)([^/]*)/(.*$)"

    @classmethod
    def _extract_s3_folder(cls, uri):
        return re.search(cls.S3_URI_PATTERN, uri).group(3)

    @classmethod
    def _extract_s3_bucket(cls, uri):
        return re.search(cls.S3_URI_PATTERN, uri).group(2)

    def filter_false_positive_boto3_warning(self):
        # see
        # https://github.com/boto/boto3/issues/454#issuecomment-380900404
        import warnings

        warnings.filterwarnings("ignore", category=ResourceWarning, message="unclosed <ssl.SSLSocket")
        warnings.filterwarnings("ignore", category=DeprecationWarning, message="the imp module is deprecated")
        warnings.filterwarnings("ignore", category=DeprecationWarning, message="Using or importing the ABCs")


class AthenaSqlDialect(SqlDialect):
    # We need to pass the data source impl to the dialect to be able to access connection properties (such as the staging dir)
    def __init__(self, data_source_impl: AthenaDataSourceImpl):
        super().__init__()
        self.data_source_impl = data_source_impl

    def get_contract_type_dict(self) -> dict[str, str]:
        base_contract_type_dict = super().get_contract_type_dict()
        base_contract_type_dict[DBDataType.TEXT] = "varchar"
        base_contract_type_dict[DBDataType.BOOLEAN] = "boolean"
        base_contract_type_dict[DBDataType.INTEGER] = "integer"
        base_contract_type_dict[DBDataType.DECIMAL] = "decimal"
        base_contract_type_dict[DBDataType.DATE] = "date"
        base_contract_type_dict[DBDataType.TIME] = "date"
        base_contract_type_dict[DBDataType.TIMESTAMP] = "timestamp(3)"
        base_contract_type_dict[DBDataType.TIMESTAMP_TZ] = "timestamp(3)"
        return base_contract_type_dict

    def get_sql_type_dict(self) -> dict[str, str]:
        base_sql_type_dict = super().get_sql_type_dict()
        base_sql_type_dict[DBDataType.TEXT] = "string"
        base_sql_type_dict[DBDataType.INTEGER] = "int"
        base_sql_type_dict[DBDataType.TIMESTAMP] = "timestamp"
        base_sql_type_dict[DBDataType.TIMESTAMP_TZ] = "timestamp"
        return base_sql_type_dict

    def quote_default(self, identifier: Optional[str]) -> Optional[str]:
        return identifier

    def quote_column(self, column_name: str) -> str:
        return f'"{column_name}"'

    def _build_column_sql(self, column: COLUMN) -> str:
        table_alias_sql: str = f"{self.quote_default(column.table_alias)}." if column.table_alias else ""
        # We need to check if the column name is a string (then quote it ourselves) or a SqlExpression (then let it be compiled)
        if isinstance(column.name, str):
            column_sql = f'"{column.name}"'
        else:
            column_sql = self.build_expression_sql(column.name)
        field_alias_sql: str = f" AS {self.quote_default(column.field_alias)}" if column.field_alias else ""
        return f"{table_alias_sql}{column_sql}{field_alias_sql}"

    def literal_datetime(self, datetime: datetime):
        return f"From_iso8601_timestamp('{datetime.isoformat()}')"

    def literal_datetime_with_tz(self, datetime: datetime):
        # Can be overloaded if the subclass does not support timezones (may have to do conversion yourself)
        # We assume that all timestamps are stored in UTC.
        # See Fabric for an example
        return self.literal_datetime(datetime)

    def supports_varchar_length(self) -> bool:
        return False

    def sql_expr_timestamp_literal(self, datetime_in_iso8601: str) -> str:
        return f"CAST(From_iso8601_timestamp('{datetime_in_iso8601}') AS timestamp)"

    def sql_expr_timestamp_add_day(self, timestamp_literal: str) -> str:
        return f"{timestamp_literal} + interval '1' day"

    def supports_case_sensitive_column_names(self) -> bool:
        return False  # Athena does not support case sensitive names: everything is lowercase.

    def build_create_table_sql(
        self, create_table: CREATE_TABLE | CREATE_TABLE_IF_NOT_EXISTS, add_semicolon: bool = True
    ) -> str:
        create_table_sql = self._build_create_table_statement_sql(create_table)

        create_table_sql = (
            create_table_sql
            + "(\n"
            + ",\n".join([self._build_create_table_column(column) for column in create_table.columns])
            + "\n)"
        )
        location = self.data_source_impl.table_s3_location(create_table.fully_qualified_table_name)
        create_table_sql = create_table_sql + f"\nLOCATION '{location}'"
        return create_table_sql + (";" if add_semicolon else "")

    def _build_create_table_statement_sql(self, create_table: CREATE_TABLE | CREATE_TABLE_IF_NOT_EXISTS) -> str:
        if_not_exists_sql: str = "IF NOT EXISTS" if isinstance(create_table, CREATE_TABLE_IF_NOT_EXISTS) else ""
        create_table_sql: str = f"CREATE EXTERNAL TABLE {if_not_exists_sql} {create_table.fully_qualified_table_name} "
        return create_table_sql

    def _quote_column_for_create_table(self, column_name: str) -> str:
        return f"`{column_name}`"

    def _is_not_null_ddl_supported(self) -> bool:
        return False
