import logging
import re
from datetime import datetime
from typing import Any, Optional, Tuple

import boto3
from soda_athena.common.data_sources.athena_data_source_connection import (
    AthenaDataSource as AthenaDataSourceModel,
)
from soda_athena.common.data_sources.athena_data_source_connection import (
    AthenaDataSourceConnection,
)
from soda_core.common.data_source_connection import DataSourceConnection
from soda_core.common.data_source_impl import DataSourceImpl
from soda_core.common.data_source_results import UpdateResult
from soda_core.common.logging_constants import soda_logger
from soda_core.common.metadata_types import SodaDataTypeName, SqlDataType
from soda_core.common.sql_ast import (
    COLUMN,
    CREATE_TABLE,
    CREATE_TABLE_COLUMN,
    CREATE_TABLE_IF_NOT_EXISTS,
)
from soda_core.common.sql_dialect import SqlDialect

logger: logging.Logger = soda_logger


class AthenaDataSourceImpl(DataSourceImpl, model_class=AthenaDataSourceModel):
    def __init__(self, data_source_model: AthenaDataSourceModel):
        super().__init__(data_source_model=data_source_model)

    def _create_sql_dialect(self) -> SqlDialect:
        return AthenaSqlDialect(self)

    def _create_data_source_connection(self) -> DataSourceConnection:
        return AthenaDataSourceConnection(
            name=self.data_source_model.name, connection_properties=self.data_source_model.connection_properties
        )

    # def execute_query(self, sql: str) -> QueryResult:
    #     # Athena does not play well with freezegun.
    #     # The datasources requires a timestamp in the query (done automatically), this timestamp must be in sync with AWS servers
    #     # This is not the case when using freezegun.
    #     # We need to disable freezegun for this method.

    #     # TODO: Refactor this so it's in a proper place. It shouldn't live here.
    #     # now = datetime.now()
    #     # if now < datetime(year=2025, month=8, day=1, hour=0, minute=0, second=0):
    #     #     with freezegun.freeze_time(freezegun.api.real_datetime.now(tz=timezone.utc)):
    #     #         result = self.connection.execute_query(sql=sql)
    #     #     return result
    #     return self.connection.execute_query(sql=sql)

    def execute_update(self, sql: str) -> UpdateResult:
        result: UpdateResult = super().execute_update(sql)
        # Athena requires some extra cleanup for a drop table or drop schema statement.
        if sql.startswith("DROP TABLE "):
            # We assume that the statement is generated by the SQL Dialect, so we can parse the table name from it.
            qualified_table_name = self.__parse_table_name_from_drop_table_sql(sql)
            table_location = self.table_s3_location(qualified_table_name)
            self._delete_s3_files(table_location)
        elif sql.startswith("DROP SCHEMA"):
            pass  # This is for later, if we ever support dropping schemas in the SQL AST.
        return result

    def table_s3_location(self, qualified_table_name: str) -> str:
        table_part_for_location = qualified_table_name.replace(".", "/").replace('"', "")
        location = f"{self.connection.athena_staging_dir}/{table_part_for_location}/"
        return location

    def __parse_table_name_from_drop_table_sql(self, sql: str) -> str:
        remaining_sql = sql[len("DROP TABLE ") :].strip()
        if remaining_sql.startswith("IF EXISTS "):
            remaining_sql = remaining_sql[len("IF EXISTS ") :].strip()
        if remaining_sql.endswith(";"):
            remaining_sql = remaining_sql[:-1]
        return remaining_sql.strip()

    def _delete_s3_files(self, directory: str):
        """
        Deletes all files under a given directory in S3.
        """
        logger.debug(f"Deleting all s3 files under {directory}")
        bucket = self._extract_s3_bucket(directory)
        folder = self._extract_s3_folder(directory)
        s3_client = self._create_s3_client()
        response = s3_client.list_objects_v2(Bucket=bucket, Prefix=folder)
        object_keys = self._extract_object_keys(response)
        logger.debug(f"Found {len(object_keys)} to be deleted")
        max_objects = 200
        assert len(object_keys) < max_objects, (
            f"This method is intended for tests and hence limited to a maximum of {max_objects} objects, "
            f"{len(object_keys)} objects exceeds the limit."
        )
        if len(object_keys) > 0:
            response: dict = s3_client.delete_objects(Bucket=bucket, Delete={"Objects": object_keys})
            deleted_list = response.get("Deleted") if isinstance(response, dict) else None
            deleted_count = (
                len(deleted_list) if isinstance(deleted_list, list) else "Unknown aws delete objects response"
            )
            logger.debug(f"Deleted {deleted_count}")

    def _create_s3_client(self) -> boto3.client:
        self.filter_false_positive_boto3_warning()
        aws_credentials = self.connection.aws_credentials
        aws_credentials = aws_credentials.resolve_role("soda_sql_test_cleanup")
        return boto3.client(
            "s3",
            region_name=aws_credentials.region_name,
            aws_access_key_id=aws_credentials.access_key_id,
            aws_secret_access_key=aws_credentials.secret_access_key,
            aws_session_token=aws_credentials.session_token,
        )

    def _extract_object_keys(self, response: dict) -> list[dict]:
        object_keys = []
        if "Contents" in response:
            objects = response["Contents"]
            for summary in objects:
                key = summary["Key"]
                object_keys.append({"Key": key})
        return object_keys

    S3_URI_PATTERN = r"(^s3://)([^/]*)/(.*$)"

    @classmethod
    def _extract_s3_folder(cls, uri):
        return re.search(cls.S3_URI_PATTERN, uri).group(3)

    @classmethod
    def _extract_s3_bucket(cls, uri):
        return re.search(cls.S3_URI_PATTERN, uri).group(2)

    def filter_false_positive_boto3_warning(self):
        # see
        # https://github.com/boto/boto3/issues/454#issuecomment-380900404
        import warnings

        warnings.filterwarnings("ignore", category=ResourceWarning, message="unclosed <ssl.SSLSocket")
        warnings.filterwarnings("ignore", category=DeprecationWarning, message="the imp module is deprecated")
        warnings.filterwarnings("ignore", category=DeprecationWarning, message="Using or importing the ABCs")


class AthenaSqlDialect(SqlDialect):
    # We need to pass the data source impl to the dialect to be able to access connection properties (such as the staging dir)
    def __init__(self, data_source_impl: AthenaDataSourceImpl):
        super().__init__()
        self.data_source_impl = data_source_impl

    def default_casify(self, identifier: str) -> str:
        return identifier.lower()

    def metadata_casify(self, identifier: str) -> str:
        return identifier.lower()

    def get_data_source_data_type_name_by_soda_data_type_names(self) -> dict:
        """
        Maps DBDataType names to data source type names.
        """
        return {
            SodaDataTypeName.CHAR: "char",
            SodaDataTypeName.VARCHAR: "varchar",
            SodaDataTypeName.TEXT: "varchar",
            SodaDataTypeName.SMALLINT: "smallint",
            SodaDataTypeName.INTEGER: "integer",
            SodaDataTypeName.BIGINT: "bigint",
            SodaDataTypeName.DECIMAL: "decimal",
            SodaDataTypeName.NUMERIC: "decimal",
            SodaDataTypeName.FLOAT: "float",
            SodaDataTypeName.DOUBLE: "double",
            SodaDataTypeName.DATE: "date",
            SodaDataTypeName.TIME: "date",
            SodaDataTypeName.TIMESTAMP: "timestamp",
            SodaDataTypeName.TIMESTAMP_TZ: "timestamp",
            SodaDataTypeName.BOOLEAN: "boolean",
        }

    def _get_data_type_name_synonyms(self) -> list[list[str]]:
        # Implements data type synonyms
        # Each list should represent a list of synonyms
        return [
            ["varchar", "character varying", "string"],
            ["decimal"],
            ["timestamp"],
        ]

    def get_data_source_type_names_by_test_type_names(self) -> dict[str, str]:
        raise NotImplementedError()

    def quote_default(self, identifier: Optional[str]) -> Optional[str]:
        return identifier

    def quote_column(self, column_name: str) -> str:
        return f'"{column_name}"'

    def _build_column_sql(self, column: COLUMN) -> str:
        table_alias_sql: str = f"{self.quote_default(column.table_alias)}." if column.table_alias else ""
        # We need to check if the column name is a string (then quote it ourselves) or a SqlExpression (then let it be compiled)
        if isinstance(column.name, str):
            column_sql = f'"{column.name}"'
        else:
            column_sql = self.build_expression_sql(column.name)
        field_alias_sql: str = f" AS {self.quote_default(column.field_alias)}" if column.field_alias else ""
        return f"{table_alias_sql}{column_sql}{field_alias_sql}"

    def literal_datetime(self, datetime: datetime):
        return f"From_iso8601_timestamp('{datetime.isoformat()}')"

    def literal_datetime_with_tz(self, datetime: datetime):
        # Can be overloaded if the subclass does not support timezones (may have to do conversion yourself)
        # We assume that all timestamps are stored in UTC.
        # See Fabric for an example
        return self.literal_datetime(datetime)

    def supports_varchar_length(self) -> bool:
        return False

    def sql_expr_timestamp_literal(self, datetime_in_iso8601: str) -> str:
        return f"CAST(From_iso8601_timestamp('{datetime_in_iso8601}') AS timestamp)"

    def sql_expr_timestamp_add_day(self, timestamp_literal: str) -> str:
        return f"{timestamp_literal} + interval '1' day"

    def build_create_table_sql(
        self, create_table: CREATE_TABLE | CREATE_TABLE_IF_NOT_EXISTS, add_semicolon: bool = True
    ) -> str:
        create_table_sql = self._build_create_table_statement_sql(create_table)

        create_table_sql = (
            create_table_sql
            + "(\n"
            + ",\n".join([self._build_create_table_column(column) for column in create_table.columns])
            + "\n)"
        )
        location = self.data_source_impl.table_s3_location(create_table.fully_qualified_table_name)
        create_table_sql = create_table_sql + f"\nLOCATION '{location}'"
        return create_table_sql + (";" if add_semicolon else "")

    def _build_create_table_statement_sql(self, create_table: CREATE_TABLE | CREATE_TABLE_IF_NOT_EXISTS) -> str:
        if_not_exists_sql: str = "IF NOT EXISTS" if isinstance(create_table, CREATE_TABLE_IF_NOT_EXISTS) else ""
        create_table_sql: str = f"CREATE EXTERNAL TABLE {if_not_exists_sql} {create_table.fully_qualified_table_name} "
        return create_table_sql

    def _build_create_table_column_type(self, create_table_column: CREATE_TABLE_COLUMN) -> str:
        assert isinstance(create_table_column.type, SqlDataType)
        # We need to check for text datatypes if there is a length parameter. If there is, use varchar, otherwise use string.
        # This only applies to the create table statement, not the schema check. For the schema check we need to use the original data type (varchar), but without length.
        if create_table_column.type.name == "varchar" and create_table_column.type.character_maximum_length is None:
            return "string"
        else:
            return create_table_column.type.get_sql_data_type_str_with_parameters()

    def _quote_column_for_create_table(self, column_name: str) -> str:
        return f"`{column_name}`"

    def _is_not_null_ddl_supported(self) -> bool:
        return False

    def supports_data_type_character_maximum_length(self) -> bool:
        return True

    def supports_data_type_numeric_precision(self) -> bool:
        return True

    def supports_data_type_numeric_scale(self) -> bool:
        return True

    def supports_data_type_datetime_precision(self) -> bool:
        return False  # Technically it is supported, but we can't modify it in a CREATE TABLE statement (always defaults to 3)

    def column_data_type_max_length(self) -> Optional[str]:
        """Athena supports this but it's not in information schema."""
        return None

    def column_data_type_numeric_precision(self) -> Optional[str]:
        """Athena supports this but it's not in information schema."""
        return None

    def column_data_type_numeric_scale(self) -> Optional[str]:
        """Athena supports this but it's not in information schema."""
        return None

    def format_metadata_data_type(self, data_type: str) -> str:
        """Athena sometimes modifies data types to include precision (e.g. TIMESTAMP as TIMESTAMP(3)) in column metadata

        We don't want data type comparisons to fail, so strip this extra information.
        """
        paranthesis_index = data_type.find("(")
        if paranthesis_index != -1:
            return data_type[:paranthesis_index]
        return data_type

    def data_type_has_parameter_character_maximum_length(self, data_type_name) -> bool:
        return self.format_metadata_data_type(data_type_name).lower() in ["varchar", "char"]

    def data_type_has_parameter_numeric_precision(self, data_type_name) -> bool:
        return self.format_metadata_data_type(data_type_name).lower() in ["decimal"]

    def data_type_has_parameter_numeric_scale(self, data_type_name) -> bool:
        return self.format_metadata_data_type(data_type_name).lower() in ["decimal"]

    def data_type_has_parameter_datetime_precision(self, data_type_name) -> bool:
        return self.format_metadata_data_type(data_type_name).lower() in ["timestamp", "time"]

    def supports_case_sensitive_column_names(self) -> bool:
        return False  # Athena does not support case sensitive names: everything is lowercase.

    def extract_character_maximum_length(self, row: Tuple[Any, ...], columns: list[Tuple[Any, ...]]) -> Optional[int]:
        # Varchars are a special case, they may contain a length parameter, but not always!
        data_type_name: str = self.extract_data_type_name(row, columns)
        formatted_data_type_name: str = self.format_metadata_data_type(data_type_name)
        if not self.data_type_has_parameter_character_maximum_length(data_type_name):
            return None
        try:
            # extract value from inside parentheses
            data_type_tuple = data_type_name[len(formatted_data_type_name) + 1 : -1].split(",")
            return int(data_type_tuple[0])
        except ValueError:
            return None

    def extract_numeric_precision(self, row: Tuple[Any, ...], columns: list[Tuple[Any, ...]]) -> Optional[int]:
        data_type_name: str = self.extract_data_type_name(row, columns)
        formatted_data_type_name: str = self.format_metadata_data_type(data_type_name)
        if not self.data_type_has_parameter_numeric_precision(data_type_name):
            return None

        formatted_data_type_name: str = self.format_metadata_data_type(data_type_name)
        data_type_tuple = data_type_name[len(formatted_data_type_name) + 1 : -1].split(",")
        return int(data_type_tuple[0])

    def extract_numeric_scale(self, row: Tuple[Any, ...], columns: list[Tuple[Any, ...]]) -> Optional[int]:
        data_type_name: str = self.extract_data_type_name(row, columns)
        formatted_data_type_name: str = self.format_metadata_data_type(data_type_name)
        if not self.data_type_has_parameter_numeric_scale(data_type_name):
            return None

        data_type_tuple = data_type_name[len(formatted_data_type_name) + 1 : -1].split(",")
        return int(data_type_tuple[1])
