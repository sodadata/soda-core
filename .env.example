# Uncomment any of the following to run a test locally on those data sources.
# postgres is the default so that's used if all these below are commented.
# test_data_source=postgres
# test_data_source=snowflake
# test_data_source=bigquery
# test_data_source=redshift
# test_data_source=athena
# test_data_source=spark
# test_data_source=spark_df

# Flag to skip scientific tests if installation doesn't work on your machine
# SCIENTIFIC_TESTS="SKIP"

# This flag allows to skip the optimized local behavior and run on postgres with a clean schema, just like on CI.
# By default running the test suite on your own dev machine will skip dropping the schema and test tables will be
# reused when running them next time. Of course, if the data in the test tables change, the test tables will be
# recreated.
# POSTGRES_REUSE_SCHEMA=DISABLED

# Creds for dev.sodadata.io if you want to use the manual integration tests in soda/core/tests/integration
DEV_SODADATA_IO_API_KEY_ID=***
DEV_SODADATA_IO_API_KEY_SECRET=***

BIGQUERY_ACCOUNT_INFO_JSON_PATH=/Users/you/.soda/my_bigquery_account_info.json
BIGQUERY_DATASET=test

SNOWFLAKE_HOST=https://***.eu-central-1.snowflakecomputing.com/
SNOWFLAKE_ACCOUNT=***.eu-central-1
SNOWFLAKE_USERNAME=***
SNOWFLAKE_PASSWORD=***
SNOWFLAKE_DATABASE=***
SNOWFLAKE_SCHEMA=public

REDSHIFT_HOST=***
REDSHIFT_USERNAME=***
REDSHIFT_PASSWORD=***
REDSHIFT_DATABASE=***
REDSHIFT_PORT=5439

ATHENA_ACCESS_KEY_ID=***
ATHENA_SECRET_ACCESS_KEY=***
ATHENA_S3_TEST_DIR=s3://***/
ATHENA_SCHEMA=***

# Create and test checks with views instead of tables
TEST_WITH_VIEWS=false

CONTRACTS_POSTGRES_HOST=***
CONTRACTS_POSTGRES_USERNAME=***
CONTRACTS_POSTGRES_PASSWORD=***
CONTRACTS_POSTGRES_DATABASE=***

ATLAN_API_KEY=***

FABRIC_ENDPOINT=***
FABRIC_DWH=***
